Sparkflows Pipeline - CPG Store Segmentation
=================================

.. contents::
   :depth: 3

Pipeline Overview
-------------------

**Pipeline** helps in orchestrating execution of workflows to achieve a business function such as Store Segmentation.

Processing steps pass through the sequence of workflows in a linear manner. Fire Insights provides a mechanism to create, execute and schedule Pipelines, thus helping automate executions.

It enhances the implementation experience of Data Engineering, Data Science, CI/CD and various Machine Learning use cases. Along with orchestration, the pipeline also extends the capability to define parameters in a single window, which can be used by the child workflows. It also facilitates adding Email Ids that can be notified on success or failure of pipeline execution.

Sparkflows Pipeline Tutorials
-------------------

This tutorial will cover how to use Sparkflows Pipeline to automate CPG Store Segmentation processing. 

The aim of this tutorial is to create a pipeline encompassing steps like Performing Invoice Data Exploration, Data Cleaning, Data Preparation and Store Segmentation.

The Objective of CPG Store Segmentation project is grade stores based on various attributes such as sales, the kinds of products they sell, and so on.

This tutorial covers steps involved in Pipeline Creation, Pipeline Execution and Pipeline Scheduling. It uses synthetic data prepared in-house.  

It involves following Datasets and Workflows:

* Datasets

	* **Customer Invoice:** It contains invoices generated by various Customers over a period of time.

	* **Product Mapping:** It contains the mapping information between the Product and its Parent.

* Workflows

	* **Exploratory Data Analysis** - It computes and displays various metrics such as Summary Statistics, Null Values in the dataset, Columns Cardinality  			and Monthly Invoice Count.

	* **Data Cleaning** - It performs data cleaning tasks on Customer Invoice data such as converting string to date values, extracting date parts and 				saves the cleaned data. It also joins invoice data with product mapping data.

	* **Data Preparation** - It implements various aggregation techniques to prepare and compute features such as Avg Monthly Purchases, Avg Invoice Purchases, 		Count of Unique Products in an Invoice, Total Number of Invoices and so on.

	* **Store Segmentation** - It implements k-means clustering algorithm for Store Segmentation. 

Pipeline Creation
^^^^^^^^^^^^^^^^^^^^^^^^

Various Store Segmentation workflows are stitched together to create Store Segmentation Pipeline. It is implemented using various workflows, as shown below.
	  
   .. figure:: ../../_assets/tutorials/pipeline/pipeline-tutorials-cs-sspipeline.png
      :alt: pipeline-tutorials
      :width: 70%

* It executes all the workflows sequentially as per the definition.
* It is advisable to add workflows as per the business processing steps.

Addition of Workflows
^^^^^^^^^^^^^^^^^^^^^^^^

Workflows can be added to a Pipeline step using the **Workflow Node**, as shown below.

   .. figure:: ../../_assets/tutorials/pipeline/pipeline-tutorials-cs-wfnode.png
      :alt: pipeline-tutorials
      :width: 70%

* It executes the mapped workflow.
* All the four workflows need to be added using the **Workflow Node**. They need to be connected using **Connector**, as shown above.

Pipeline Execution And Scheduling
------------------

A **Sparkflows Pipeline** can be executed in the following two ways:

* Manual Execution
* Triggering using a Pipeline Schedule

Manual Execution
^^^^^^^^^^^^^^^^^^^^^

Click the **Execute** button to execute the pipeline manually, as shown below. 

 .. figure:: ../../_assets/tutorials/pipeline/pipeline-tutorials-cs-manualexec.png
    :alt: Pipeline Tutorials
    :width: 70%

* It will execute the Child Workflows in sequence as defined. 
* The execution details can be viewed on the **Pipeline Execution** page.
* It picks up the latest data from the datafile for processing.

Triggering using a Pipeline Schedule
^^^^^^^^^^^^^^^^^^^^^

A **Sparkflows Pipeline** can also be triggered at a specified time interval based on the defined **Pipeline Schedule**.

Suppose, if updated Invoice data is received everyday at 09:00 am in the morning, this pipeline can be scheduled to execute everyday at 09:30 am, ensuring that Store Segmentation is performed everyday on the latest data.

The Store Segmentation Pipeline can be scheduled as shown below. 

 .. figure:: ../../_assets/tutorials/pipeline/pipeline-tutorials-cs-newschedule.png
    :alt: Pipeline Tutorials
    :width: 70%

* It will execute the pipeline everyday at 9:30 am.
* It  will execute all the workflows in sequence using the new data. 

Pipeline Execution Status
---------------------------

To view the pipeline execution status, follow these steps:

#. Navigate to **Pipelines > Executions**.
#. The **execution status** of your pipeline will be displayed on the page shown below.

   .. figure:: ../../_assets/tutorials/pipeline/pipeline-tutorials-cs-execstatus.png
      :alt: Pipeline Tutorials
      :width: 60%

   **Launched:** It can be used to identify how a **Pipeline** has been executed - either **Manually** or using **Scheduler**.

#. Click on the **Pipeline Name** to view the execution status of individual workflows as shown below.

   .. figure:: ../../_assets/tutorials/pipeline/pipeline-tutorials-cs-wfexecstatus.png
      :alt: Pipeline Tutorials
      :width: 60%
