Sparkflows Pipeline - CPG Store Segmentation
=================================

This tutorial will cover how to use Sparkflows Pipeline to automate CPG Store Segmentation processing. 

The aim of this tutorial is to create a pipeline encompassing steps like Performing Invoice Data Exploration, Data Cleaning, Data Preparation and Store Segmentation.

The Objective of CPG Store Segmentation project is grade stores based on various attributes such as sales, the kinds of products they sell, and so on.

It uses synthetic data prepared in-house.  

.. contents::
   :depth: 2

Dataset Creation
-------------------

The following two datasets have been used in the Tutorial. They have been created on top of in-house synthetic data, as shown below.

* Customer Invoice: It contains invoices generated by various Customers over a period of time.
* Product Mapping: It contains the mapping information between the Product and its Parent.

  .. figure:: ../../_assets/tutorials/pipeline/pipeline-tutorials-cs-datasets.png
     :alt: pipeline-tutorials
     :width: 70%

Customer Invoice
^^^^^^^^^^^^^^^^^^^^^^^^

Customer Invoice Dataset has following columns. 

   .. figure:: ../../_assets/tutorials/pipeline/pipeline-tutorials-cs-invoiceds.png
      :alt: pipeline-tutorials
      :width: 70%
	  
Product Mapping
^^^^^^^^^^^^^^^^^^^^^^^^

Product Mapping Dataset has following columns.
	  
   .. figure:: ../../_assets/tutorials/pipeline/pipeline-tutorials-cs-prdmappingds.png
      :alt: pipeline-tutorials
      :width: 70%

Workflows Overview
-------------------

The following set of workflows are used in the Store Segmentation Project: 

Exploratory Data Analysis
^^^^^^^^^^^^^^^^^^^^^^^^

This workflow facilitates performing data analysis on Customer Invoice data. It is implemented using various Sparkflows processors, as shown below.
	  
   .. figure:: ../../_assets/tutorials/pipeline/pipeline-tutorials-cs-edawf.png
      :alt: pipeline-tutorials
      :width: 70%

* It reads data from Customer Invoice Dataset.
* It computes and displays various metrics such as Summary Statistics, Null Values in the dataset, Columns Cardinality and Monthly Invoice Count.

Data Cleaning
^^^^^^^^^^^^^^^^^^^^^^^^

This workflow facilitates performing data cleaning tasks. It is implemented using various Sparkflows processors, as shown below.
	  
   .. figure:: ../../_assets/tutorials/pipeline/pipeline-tutorials-cs-dcwf.png
      :alt: pipeline-tutorials
      :width: 70%

* It reads data from Customer Invoice Dataset.
* It performs data cleaning tasks such as converting string to date values, extracting date parts and saves the cleaned data.
* It joins invoice data with product mapping data.
* The cleaned data from this stage would be used for further processing.

Data Preparation
^^^^^^^^^^^^^^^^^^^^^^^^

This workflow facilitates performing data preparation tasks. It is implemented using various Sparkflows processors, as shown below.
	  
   .. figure:: ../../_assets/tutorials/pipeline/pipeline-tutorials-cs-dpwf.png
      :alt: pipeline-tutorials
      :width: 70%

* It reads the cleaned data saved in the last stage.
* It implements various aggregation techniques to prepare and compute features that will be used during segmentation. 
* It computes features such as Avg Monthly Purchases, Avg Invoice Purchases, Count of Unique Products in an Invoice, Total Number of Invoices and so on.
* The prepared feature data is saved for further use during segmentation stage.

Store Segmentation
^^^^^^^^^^^^^^^^^^^^^^^^

This workflow facilitates performing store segmentaion. It is implemented using various Sparkflows processors, as shown below.
	  
   .. figure:: ../../_assets/tutorials/pipeline/pipeline-tutorials-cs-sswf.png
      :alt: pipeline-tutorials
      :width: 70%

* It reads the feature data saved in the last stage.
* It implements k-means clustering algorithm for Store Segmentation. 

Pipeline Overview
-------------------

**Pipeline** helps in orchestrating execution of workflows to achieve a business function such as Store Segmentation.

Processing steps pass through the sequence of workflows in a linear manner. Fire Insights provides a mechanism to create, execute and schedule Pipelines, thus helping automate executions.

It enhances the implementation experience of Data Engineering, Data Science, CI/CD and various Machine Learning use cases. Along with orchestration, the pipeline also extends the capability to define parameters in a single window, which can be used by the child workflows. It also facilitates adding Email Ids that can be notified on success or failure of pipeline execution.

Pipeline Creation
^^^^^^^^^^^^^^^^^^^^^^^^

Various Store Segmentation workflows are stitched together to create Store Segmentation Pipeline. It is implemented using various workflows, as shown below.
	  
   .. figure:: ../../_assets/tutorials/pipeline/pipeline-tutorials-cs-sspipeline.png
      :alt: pipeline-tutorials
      :width: 70%

* It executes all the workflows sequentially as per the definition.
* It is advisable to add workflows as per the business processing steps.

Addition of Workflows
^^^^^^^^^^^^^^^^^^^^^^^^

Workflows can be added to a Pipeline step using the **Workflow Node**, as shown below.

   .. figure:: ../../_assets/tutorials/pipeline/pipeline-tutorials-cs-wfnode.png
      :alt: pipeline-tutorials
      :width: 70%

* It executes the mapped workflow.
* All the four workflows need to be added using the **Workflow Node**. They need to be connected using **Connector**, as shown above.

Pipeline Execution And Scheduling
------------------

A **Sparkflows Pipeline** can be executed in the following two ways:

* Manual Execution
* Triggering using a Pipeline Schedule

Manual Execution
^^^^^^^^^^^^^^^^^^^^^

Click the **Execute** button to execute the pipeline manually, as shown below. 

 .. figure:: ../../_assets/tutorials/pipeline/pipeline-tutorials-cs-manualexec.png
    :alt: Pipeline Tutorials
    :width: 70%

* It will execute the Child Workflows in sequence as defined. 
* The execution details can be viewed on the **Pipeline Execution** page.
* It picks up the latest data from the datafile for processing.

Triggering using a Pipeline Schedule
^^^^^^^^^^^^^^^^^^^^^

A **Sparkflows Pipeline** can also be triggered at a specified time interval based on the defined **Pipeline Schedule**.

Suppose, if updated Invoice data is received everyday at 09:00 am in the morning, this pipeline can be scheduled to execute everyday at 09:30 am, ensuring that Store Segmentation is performed everyday on the latest data.

The Store Segmentation Pipeline can be scheduled as shown below. 

 .. figure:: ../../_assets/tutorials/pipeline/pipeline-tutorials-cs-newschedule.png
    :alt: Pipeline Tutorials
    :width: 70%

* It will execute the pipeline everyday at 9:30 am.
* It  will execute all the workflows in sequence using the new data. 

